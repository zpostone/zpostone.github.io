{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "df_urls = pd.DataFrame()\n",
    "df_urls = pd.read_table('urls_1.csv', sep=',')\n",
    "#df_test = df_urls.iloc[0:10, :]\n",
    "df_test = df_urls.loc[[536,115], :]\n",
    "#df_test = df_urls.head(n=2)\n",
    "\n",
    "df_test\n",
    "def getEmail(x):\n",
    "    page = requests.get(str(x))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", soup.text, re.I))\n",
    "    #mailtos = soup.select('a[href^=mailto]')\n",
    "    #return soup\n",
    "    EmailSearch = [emails,soup]\n",
    "    return EmailSearch\n",
    "    \n",
    "    \n",
    "for index, row in df_test.iterrows():\n",
    "    x = row['URLS']\n",
    "    EmailSearch2 = getEmail(x)\n",
    "    emails = EmailSearch2[0]\n",
    "    soup = EmailSearch2[1]\n",
    "    print (emails)\n",
    "    #if emails==set():\n",
    "        #print(\"EMAIL EQUALS SET\")\n",
    "        #for elem in soup(text=re.compile(r'Contact')):\n",
    "            #parent = (elem.parent)\n",
    "            #y = parent.get('href')\n",
    "            #print(y)\n",
    "            #EmailSearch3 = getEmail(y)\n",
    "            #emails = EmailSearch3[0]\n",
    "            \n",
    "    #df_test.set_value(index, 'EMAIL', emails)\n",
    "    #df_test.set_value(index, 'MAILTOS', mailtos)\n",
    "\n",
    "df_test\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#for index, row in df_test.iterrows():\n",
    "    #x = row['URLS']\n",
    "    #getEmail(x)\n",
    "    #if emails == \"\" and mailtos ==\"\":\n",
    "    #this seems to crash when letting more than a few rows into the df\n",
    "\n",
    "    #print (emails)\n",
    "    #print (index)\n",
    "    #print (row)\n",
    "    #df_test.set_value(index, 'EMAIL', emails)\n",
    "    #df_test.set_value(index, 'MAILTOS', mailtos)\n",
    "    #How to write the email list values into multiple new columns next to the URL?\n",
    "    #if df_test['EMAIL'] == \" \":\n",
    "        #df_test.set_value(index, 'EMAIL', 'NO EMAIL')\n",
    "    \n",
    "    #for elem in soup(text=re.compile(r'Contact')):\n",
    "        #parent = (elem.parent)\n",
    "        #x = parent.get('href')\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #for link in soup.find_all('a'):\n",
    "    #for link in item.find_all('a'):\n",
    "        #FIND A WAY TO ONLY DO 'a' where the text matches \"contact\" or \"about us\" or \"contact\" etc.\n",
    "        #print (link.get('href'))\n",
    "        #pass that href back into the email scraper\n",
    "        #repeat the analysis\n",
    "        \n",
    "        #http://stackoverflow.com/a/14470573\n",
    "\n",
    "#page = requests.get('https://northstarfund.org/')\n",
    "#soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print (soup)        \n",
    "        \n",
    "#page = requests.get(str(row['URLS']))\n",
    "    \n",
    "    \n",
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# WORKING\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os, json\n",
    "from bs4 import BeautifulSoup\n",
    "#import lxml\n",
    "\n",
    "df_urls = pd.DataFrame()\n",
    "df_urls = pd.read_table('urls_1.csv', sep=',')\n",
    "\n",
    "#for row in df_urls:\n",
    "#make this iterate through the rows in df_urls and append email addresses in an adjacent column. If multiple email addresses on page, add multiple columns?\n",
    "\n",
    "page = requests.get('http://dusp.mit.edu/department/contact-directions')\n",
    "tree = html.fromstring(page.text)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#mailtos = soup.select('a[href^=mailto]')\n",
    "\n",
    "# a set of crawled emails\n",
    "emails = set()\n",
    "new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", soup.text, re.I))\n",
    "#how to include mailto links (with non-email text) as another thing to capture?\n",
    "#print (mailtos)\n",
    "#print (other)\n",
    "\n",
    "print (new_emails)\n",
    "\n",
    "#soup.body.findAll(text=re.compile('^@$'))\n",
    "#for elem in soup(text=re.compile(r'@')):\n",
    "    #print (elem.parent)\n",
    "    \n",
    "#http://stackoverflow.com/a/866050    \n",
    "    \n",
    "#soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#para = soup.find_all('p')\n",
    "#print (para)\n",
    "#print(soup.prettify())\n",
    "\n",
    "#append the results from emails to new columns\n",
    "\n",
    "\n",
    "#print (tree)\n",
    "#print (buyers)\n",
    "#for i in tree:\n",
    "    #emails = re.search(\"a\", i)\n",
    "    #print (emails)\n",
    "\n",
    "#http://stackoverflow.com/a/3655588\n",
    "\n",
    "#http://scraping.pro/simple-email-crawler-python/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests.exceptions\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from collections import deque\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "df_urls = pd.DataFrame()\n",
    "df_urls = pd.read_table('urls_7.csv', sep=',')\n",
    "df_test = df_urls.head(n=2)\n",
    "\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    inputurl = (row['Links'])\n",
    "    new_urls = deque([inputurl])\n",
    "    # a set of urls that we have already crawled\n",
    "    processed_urls = set()\n",
    "    # a set of crawled emails\n",
    "    emails = set()\n",
    "    # process urls one by one until we exhaust the queue \n",
    "    while len(new_urls):\n",
    "        # move next url from the queue to the set of processed urls\n",
    "        url = new_urls.popleft()\n",
    "        processed_urls.add(url)\n",
    "        # extract base url to resolve relative links\n",
    "        parts = urllib.parse.urlsplit(url)\n",
    "        base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "        path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "        # path is full path... http://...\n",
    "\n",
    "        # get url's content\n",
    "        #print(\"Processing %s\" % url)\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "            # ignore pages with errors\n",
    "            continue\n",
    "\n",
    "        # extract all email addresses and add them into the resulting set\n",
    "        new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I))\n",
    "        emails.update(new_emails)\n",
    "\n",
    "        # create a beautiful soup for the html document\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # find and process all the anchors in the document\n",
    "        for anchor in soup.find_all(\"a\"):\n",
    "            # extract link url from the anchor\n",
    "            link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "            # resolve relative links\n",
    "            if link.startswith('mailto'):\n",
    "                emails.update(link)\n",
    "                break\n",
    "            elif link.startswith('/'):\n",
    "                link = base_url + link\n",
    "            elif not link.startswith(inputurl):\n",
    "                break\n",
    "            # add the new url to the queue if it was not enqueued nor processed yet\n",
    "            if not link in new_urls and not link in processed_urls:\n",
    "                new_urls.append(link)\n",
    "        new_emailsStr = str(new_emails)\n",
    "        df_test.set_value(index, 'Email', new_emailsStr)\n",
    "        #df_test.set_value(index, 'MAILTOS', mailtos)   \n",
    "        #print new_urls\n",
    "df_test\n",
    "\n",
    "#print new_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Links</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://boggscenter.org/</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://equitytrust.org/</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mass-ave.org/</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://projectsouth.org</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://ujimacoinc.org</td>\n",
       "      <td>{'ujimacoinc@me.com'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.cflsp.org</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://www.self-help.org</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://1lovemovement.org/</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://1worker1vote.org</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://350deschutes.org/</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Links                  Email\n",
       "0    http://boggscenter.org/                  set()\n",
       "1    http://equitytrust.org/                  set()\n",
       "2       http://mass-ave.org/                  set()\n",
       "3    http://projectsouth.org                  set()\n",
       "4      http://ujimacoinc.org  {'ujimacoinc@me.com'}\n",
       "5       http://www.cflsp.org                  set()\n",
       "6   http://www.self-help.org                  set()\n",
       "7  http://1lovemovement.org/                  set()\n",
       "8    http://1worker1vote.org                  set()\n",
       "9   http://350deschutes.org/                  set()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests.exceptions\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from collections import deque\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "df_urls = pd.DataFrame()\n",
    "df_urls = pd.read_table('urls_7.csv', sep=',')\n",
    "df_test = df_urls.head(n=10)\n",
    "\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    url = (row['Links'])\n",
    "    #new_urls = deque([inputurl])\n",
    "    # a set of urls that we have already crawled\n",
    "    processed_urls = set()\n",
    "    # a set of crawled emails\n",
    "    emails = set()\n",
    "    # process urls one by one until we exhaust the queue \n",
    "    # extract base url to resolve relative links\n",
    "    parts = urllib.parse.urlsplit(url)\n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "    # path is full path... http://...\n",
    "\n",
    "    #get url's content\n",
    "    #print(\"Processing %s\" % url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "        # ignore pages with errors\n",
    "        continue\n",
    "\n",
    "    # extract all email addresses and add them into the resulting set\n",
    "    new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I))\n",
    "    emails.update(new_emails)\n",
    "\n",
    "    # create a beautiful soup for the html document\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # find and process all the anchors in the document\n",
    "    for anchor in soup.find_all(\"a\"):\n",
    "        # extract link url from the anchor\n",
    "        link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "        # resolve relative links\n",
    "        if link.startswith('mailto'):\n",
    "            emails.update(link)\n",
    "            break\n",
    "        elif link.startswith('/'):\n",
    "            link = base_url + link\n",
    "        elif not link.startswith(inputurl):\n",
    "            break\n",
    "        # add the new url to the queue if it was not enqueued nor processed yet\n",
    "        if not link in new_urls and not link in processed_urls:\n",
    "            new_urls.append(link)\n",
    "    new_emailsStr = str(new_emails)\n",
    "    df_test.set_value(index, 'Email', new_emailsStr)\n",
    "    #df_test.set_value(index, 'MAILTOS', mailtos)   \n",
    "    #print new_urls\n",
    "\n",
    "#df_test.to_csv(EXPORT, sep=',', encoding='utf-16')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
